{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Session 1:\n",
    "## Machine learning recap and sampling\n",
    "\n",
    "*Andreas Bjerre-Nielsen*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. [Who teach](#The-teachers) and [what is this course](#This-course)\n",
    "2. [The why, what and how of machine learing](#Why-machine-learning) - see SDS L11/L14\n",
    "3. Recap of ML\n",
    "    - [Regularization](#Regularization) - see SDS L12\n",
    "    - [Model building](#Model-building) - see SDS L13\n",
    "    - [Model validation](#Model-validation) - see SDS L13/L14    \n",
    "4. [Generalization error](#Generalization-error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About Andreas\n",
    "\n",
    "Assistant professor & Head of studies (MSc Social Data Science)\n",
    "\n",
    "Research topics\n",
    "- social networks and influence\n",
    "- school choice and education\n",
    "- human behavior\n",
    "\n",
    "Check out [my website](https://abjer.github.io/) or [my Twitter profile](https://twitter.com/andbjn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About Ulf and Kristian\n",
    "\n",
    "Ulf \n",
    "- Post doc at [SODAS](sodas.ku.dk)\n",
    "- Research topics:  networks (social, biological), mobility patterns\n",
    "- Check out [Ulf's website](https://ulfaslak.com/about.html) \n",
    "\n",
    "Kristian\n",
    "- PhD student at [CEBI](https://www.econ.ku.dk/cebi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course motivation\n",
    "\n",
    "The course has two teaching teaching agendas:\n",
    "- Part 1: Machine learning and econometrics\n",
    "    - Advanced machine learning (inference, tree- and kernel based model )\n",
    "    - Combination with econometrics\n",
    "- Part 2: Networks and relations data\n",
    "    - Handle complex networks: friendships, banks, and much more..   \n",
    "    - Investigate spatial relations and objects \n",
    "    - Estimate models\n",
    "\n",
    "\n",
    "This course has synergies with other fields: \n",
    "- Economics: game theory, mechanism design, applied econometric policy evaluation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exam\n",
    "\n",
    "- Not project based!\n",
    "- Individual exam, 24 hour take home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why use models?\n",
    "### (that are not causal...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling \n",
    "*Why are models useful?*\n",
    "\n",
    "Models are pursued with differens aims. Suppose we have a regression model, $y=X\\beta+\\epsilon$.\n",
    "\n",
    "- Social science:\n",
    "    - They teach us something about the world.\n",
    "    - We want unbiased estimate $\\hat{\\beta}$ and distribution\n",
    "- Data science:\n",
    "    - To make optimal future decisions and precise predictions, i.e. $\\hat{y}$.    \n",
    "    - Model flexibility\n",
    "        - Universal Approximation (e.g. for handwriting recognition)\n",
    "    - Secondary agenda: causality (Judea Pearl etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling (2)\n",
    "Which street is from a wealthy neighborhood?\n",
    "\n",
    "Street A | Street B\n",
    "- | - \n",
    "![alt](https://github.com/abjer/tsds/raw/master/material/1_intro_sampling/backyard-2116576_1280.jpg) | ![alt](https://github.com/abjer/tsds/raw/master/material/1_intro_sampling/luxury-home-2953371_1280.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling (3)\n",
    "\n",
    "Do you think machine can learn this difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Naik, N., Kominers, S. D., Raskar, R., Glaeser, E. L., & Hidalgo, C. A. (2017). Computer vision uncovers predictors of physical urban change. Proceedings of the National Academy of Sciences, 114(29), 7571-7576."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of modelling (4)\n",
    "*Why the hype about machine learning in Social Science?*\n",
    "- Deep ideas: model validation, non-linear estimation\n",
    "- Used to construct input data\n",
    "    - E.g. parse text data, unstructured image data, network data\n",
    "- Combinination with causal methods:\n",
    "    - E.g. Causal Forest \n",
    "- Make predictions\n",
    "    - Useful in finance, macroeconomics\n",
    "    - Identifying candidates for policy that are *susceptible*\n",
    "        - likely compliers \n",
    "        - prediction policy (outcome after treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "*What do we mean by machine learning (ML)?*\n",
    "\n",
    "ML consists of two related phenomena\n",
    "\n",
    "- supervised learning\n",
    "    - assume target that is to be predicted/inferred\n",
    "    - scalar/number > regression \n",
    "    - categorical > classification \n",
    "    \n",
    "- unsupervised learning (week 3)\n",
    "    - no target for classification\n",
    "    - includes clustering, component decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning models\n",
    "Individual model\n",
    "- Linear/logistic regression  \n",
    "    - no regularization (like econometrics)\n",
    "    - with regularization (week 1/next slides)\n",
    "- Tree and kernel based methods (week 2)\n",
    "\n",
    "Combining models\n",
    "- Ensemble, bagging  (week 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (1)\n",
    "\n",
    "*Why do we regularize?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To mitigate overfitting > better model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*How do we regularize?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We make models which are less complex:\n",
    "  - reducing the **number** of coefficient;\n",
    "  - reducing the **size** of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (2)\n",
    "\n",
    "*What does regularization look like?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We add a penalty term our optimization procedure:\n",
    "    \n",
    "$$ \\text{arg min}_\\beta \\, \\underset{\\text{MSE}}{\\underbrace{E[(y_0 - \\hat{f}(x_0))^2]}} + \\underset{\\text{penalty}}{\\underbrace{\\lambda \\cdot R(\\beta)}}$$\n",
    "\n",
    "Introduction of penalties implies that increased model complexity has to be met with high increases precision of estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization (3)\n",
    "\n",
    "*What are some used penalty functions?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The two most common penalty functions are L1 and L2 regularization.\n",
    "\n",
    "- L1 regularization (***Lasso***): $R(\\beta)=\\sum_{j=1}^{p}|\\beta_j|$ \n",
    "    - Makes coefficients sparse, i.e. selects variables by removing some (if $\\lambda$ is high)\n",
    "    \n",
    "    \n",
    "- L2 regularization (***Ridge***): $R(\\beta)=\\sum_{j=1}^{p}\\beta_j^2$\n",
    "    - Reduce coefficient size\n",
    "    - Fast due to analytical solution\n",
    "    \n",
    "*To note:* The *Elastic Net* uses a combination of L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model pipelines (1)\n",
    "*Is there a smart way to build supervised ML models?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Build pipeline: \n",
    "- One step: preprocess data, estimate model\n",
    "- Ensures good practice - we only build model using training data. \n",
    "    - No data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model pipelines (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_01.png' alt=\"Drawing\" style=\"width: 900px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model performance\n",
    "*How do check our model fit?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One way is compute various measures of fit ($R^2$, accuracy etc.).\n",
    "    - Issue: adding more variable $\\Rightarrow$ higher $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*How is this solved?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use some of our sample for model evaluation. \n",
    "- Stagegy: divide into training data for estimation; remaining to test data for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification performance metrics\n",
    "\n",
    "How can we measure the performance in classification problems?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Basic idea: how many correct. But how? Many ways to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More about this in exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calibrating the model\n",
    "*Does machine learning work out of the box?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In some cases ML works quite well out of the box.\n",
    "- Often ML requires making careful choices.\n",
    "    - Note that automated machine learning packages and services exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Which choices are to be made?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We need to pick model building **hyperparameters**.\n",
    "- E.g. $\\lambda$ for Lasso, Ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation (1)\n",
    "*How do we measure our model's performance for different hyperparameters?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember we cannot use the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Could we somehow mimick what we do with test data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes, we can split the remaining non-test data into training and validation data:\n",
    "    - we train model for various hyperparameters on training data;\n",
    "    - pick the hyperparameters which performs best on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model validation (2)\n",
    "*The non-test data is split into training and validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_02.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The holdout method\n",
    "*How do we got the more out of the data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We reuse the train-test data split in reverse: \n",
    "- Rotate which parts of data is used for test and train.\n",
    "\n",
    "Advantage: We test on all the data; little extra computation.\n",
    "\n",
    "Disadvantage: Depends on the split; still only 50 pct. used for training model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Leave-one-out CV\n",
    "*How do we got the most of the data?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Procedure:\n",
    "- Each single observation as test data; remaining for training.\n",
    "- Also known as Jackknife\n",
    "\n",
    "Advantage: Robust, does not depend on random numbers!\n",
    "\n",
    "Disadvantages: \n",
    "- Very computing intensive: One model per observation. \n",
    "- Not good for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K fold method (1)\n",
    "*How do balance computing time vs. overfitting?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We split the sample into $K$ even sized test bins.\n",
    "- For each test bin $k$ we use the remaining data for training.\n",
    "\n",
    "Advantages:\n",
    "- We use all our data for testing.\n",
    "- Training is done with 100-(100/K) pct. of the data, i.e. 90 pct. for K=10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K fold method (2)\n",
    "In K-fold cross validation we average the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_03.png' alt=\"Drawing\" style=\"width: 1100px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation\n",
    "What should we do if we have more than one model that we test? Is it okay to take the one that performs best on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No, the performance of model may be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution: \n",
    "- idea: perform cross-validation (CV) multiple times on different parts of data.\n",
    "\n",
    "- **outer CV**: \n",
    "    - split data like in cross validation\n",
    "    - for each training dataset perform **inner CV** to tune hyperparameters\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (2)\n",
    "<center><img src='https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch06/images/06_07.png' alt=\"Drawing\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested cross validation (3)\n",
    "Improved measure of the uncertainty by re-doing cross-validation again and again. \n",
    "- called **Repeated k-fold Cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested resampling\n",
    "If we want to make reproducible research we should make repeated samples. Some possibilities:\n",
    "  \n",
    "- Subsampling:\n",
    "    - We randomly split data into train and test. Train data obs. are unique.    \n",
    "- The bootstrap:\n",
    "    - Draw training data with replacemtent from all data - same sample size.\n",
    "    - Unused data will be test data.\n",
    "    - Issue: Binder (2008) \"Adapting prediction error estimates for biased complexity selection in high-dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measure\n",
    "\n",
    "How do we expect our model to perform on unseen data? This is sometimes known as the **generalization error**.\n",
    "\n",
    "How can we measure the generalization error? We can compute the variability on the test set.\n",
    "\n",
    "We can use the error on the test set(s) to bound the error on unseen data \n",
    "- note: requires that we assume that new and existing data come from the **exact same distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting scientific\n",
    "\n",
    "Often we have a dataset and we want to test whether one of two models are better. We want to do this scientifically. This may be whole model building procedure including optimization of hyperparameters etc.\n",
    "\n",
    "How can do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model inference: single test set\n",
    "\n",
    "Situation: we have a model that we have trained/estimated on some part of the data (optimized etc.). The other part is NEVER tried. We test our performance on this \"new\" dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can compare the two models with standard tests, e.g. we pair observations from the models with\n",
    "- paired t-tests \n",
    "- Wilcoxon signed-rank test.\n",
    "- bootstrap / permutation stuff\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem: requires that there is **no** variability from training data \n",
    "- strong assumption, especially for non-linear e.g. tree based and neural\n",
    "- sometimes known as model stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model inference: multiple test sets\n",
    "\n",
    "If we want to make sure that we take into account model tuning uncertainty we should use more than one test set. These sets can be drawn either from:\n",
    "\n",
    "- subsampling, e.g. $n$ random, independent subsets \n",
    "- repeatedly dividing data into $k$ folds $p$ times\n",
    "\n",
    "Should we just compare the distribution of mean performance across bins (i.e. folds/subsamples)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model inference: solution with multiple test sets\n",
    "\n",
    "If we want to make sure that we take into account model tuning uncertainty we should use cross validation. How can we compare algorithms across cross validation bins?\n",
    "\n",
    "Problem: bias from training data may be too large! Nadeau and  Bengio (2003) argues that we need to correct the std. of distributions as follows.\n",
    "\n",
    "- Step 1: compute J iterations of subsampling and let the set of subsamples be denoted $\\mathcal{J}$.\n",
    "- Step 2: compute the paired mean difference, $\\hat{\\mu_j}$ for scoring function for each bin $j$.\n",
    "- Step 3: compute the mean and standard error across bins ($\\hat{\\mu}_\\mathcal{J}$ , $S^2_{\\hat{\\mu}_\\mathcal{J}}$) \n",
    "- Step 4: compute the corrected standard error across bins = ($(\\frac{1/|J|}+\\frac{n_{test}}{n_{train}})\\cdot S^2_{\\hat{\\mu}_J}$) \n",
    "\n",
    "Recommended by review in [Bouckaert, Frank (2004)](https://link.springer.com/chapter/10.1007/978-3-540-24775-3_3)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
