{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why tree based methods for econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Average Joe and beyond\n",
    "Tree based methods for \n",
    "- computing average treatment effect \n",
    "- personalized treatment effect\n",
    "\n",
    "Problem with ordinary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two approaches for heterogeneity\n",
    "\n",
    "- Data driven vs a priori sensible\n",
    "- When to choose which"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Issues with causal forest\n",
    "Limited to situation with unconfounding given covariates\n",
    "\n",
    "- Going beyond unconfounding and heterogeneity and both simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Generalized Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of trees\n",
    "\n",
    "From splitting data to computing weights\n",
    "\n",
    "<center><img src='https://raw.githubusercontent.com/abjer/sds_eml_2020/master/material/session_5/partitions_weights.JPG' alt=\"Drawing\" style=\"width: 1000px;\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overall procedure\n",
    "\n",
    "1. Repeatedly estimate trees where splits are based on estimating equation, $\\psi$ to obtain weights $\\alpha$. \n",
    "1. Re-estimate $\\psi$ using weights on entire sample where forests splits are weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating equations\n",
    "The general estimating equation\n",
    "  - $\\mathbb{E}\\left[\\psi_{\\theta(x), \\nu(x)}\\left(O_{i}\\right) | X_{i}=x\\right]=0, \\quad \\forall x.$\n",
    "    \n",
    "Where $\\psi$  estimating function, maps parameters and data into moment equations\n",
    "  - Parameters\n",
    "    - $\\theta$ parameter we want estimate \n",
    "    - $\\nu$ is nuisance we want to \"partial out\" (optional)\n",
    "  - Data     \n",
    "    - $O_i$ main objects we are interested in modelling, e.g. $Y_i, D_i$\n",
    "    - $X_i$ covariates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing weights $\\alpha$\n",
    "\n",
    "Given a subsample $\\mathcal{I}$ of data.\n",
    "\n",
    "1. Split subsample $\\mathcal{I}$ into $\\mathcal{J}_1,\\mathcal{J}_2$\n",
    "1. Estimate trees on $\\mathcal{J}_1$\n",
    "  1. Compute estimating equations on subsample at each parent node (before split)\n",
    "  1. Evaluate different splits - use approximate solutions with gradients\n",
    "1. Estimate weights using forests on $\\mathcal{J}_2$.\n",
    "  \\begin{equation}\\alpha_i(x)=\\frac{1}{B}\\sum_{b=1}\\frac{\\mathbb{1}(X_i\\in L_b(x))}{|L_b(x)|}\\end{equation}\n",
    "    - where $L_b(x)$ training examples falling in the same leaf as x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating equations\n",
    "\n",
    "Given $x$ compute the ***local*** estimating equations using weights $\\alpha_i(x)$ on entire sample:\n",
    "\n",
    "\\begin{equation}\n",
    "(\\hat{\\theta}(x), \\hat{\\nu}(x)) \\in \\operatorname{argmin}_{\\theta, \\nu}\\left\\{\\left\\|\\sum_{i=1}^{n} \\alpha_{i}(x) \\psi_{\\theta, \\nu}\\left(O_{i}\\right)\\right\\|_{2}\\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "Example of applications in Athey et al. (2019)\n",
    "- Conditional Average Treatment Effects\n",
    "- Instrumental Variables\n",
    "- Quantile Regressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation equations for CATE\n",
    "\n",
    "What does the local estimating equation look like under CATE?\n",
    "\n",
    "- the estimating equations $\\psi$ with possibly multi-dimensional treatments \n",
    "<br>\n",
    "<br>\n",
    "\\begin{align}\\psi_{\\beta(x), c(x)}\\left(Y_{i}, W_{i}\\right)=\\left(Y_{i}-\\beta(x) \\cdot W_{i}-c(x)\\right)\\left(1 \\quad W_{i}\\right)\\end{align}\n",
    "<br>\n",
    "- Note: $\\left(1 \\quad W_{i}\\right)$ which implies there is a vector of equations\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation equations for CATE (2)\n",
    "\n",
    "What is the solution?\n",
    "- Run local regression of $y_i$ on $W_i$ with weights $\\alpha$\n",
    "  \n",
    "\\begin{align}\\hat{\\theta}(x)=\\xi^{\\top}\\left(\\sum_{i=1}^{n} \\alpha_{i}(x)\\left(W_{i}-\\bar{W}_{\\alpha}\\right)^{\\otimes 2}\\right)^{-1} \\sum_{i=1}^{n} \\alpha_{i}(x)\\left(W_{i}-\\bar{W}_{\\alpha}\\right)\\left(Y_{i}-\\bar{Y}_{\\alpha}\\right)\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main results \n",
    "\n",
    "Athey et al (2019) show that Generalized Random Forests have the following propeties:\n",
    "\n",
    "- Estimates, $\\hat{\\theta}(x)$, are consistent (Theorem 3)\n",
    "- Asymptotic normality of estimates (Theorems 5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparison \n",
    "### Causal Forests and Generalized Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Causal Trees and Forests\n",
    "Strong econometric properties\n",
    "- unbiased and consistent (trees and forests)\n",
    "- asymptotic normality given $x$ (forests)\n",
    "\n",
    "- weaknesses: \n",
    "  - either unconfounding or heterogeneity\n",
    "  - we \"use\" data to buy honesty at the price of statistical power\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized Random Forest \n",
    "Difference from Causal Forest - trees are used for constructing weights!\n",
    "- strengths: \n",
    "  - unconfounding (propensity) *AND* heterogeneity\n",
    "  - additional uses \n",
    "      - quantile regression\n",
    "      - instrumental variables  \n",
    "      - clustered standard errors\n",
    "      - and more\n",
    "- weakness: \n",
    "  - we \"waste\" data on honesty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear ML for econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Treatment effects in linear models \n",
    "\n",
    "Suppose we want to estimate a linear model parameter for the causal effect of treatment $d_i$ on $y_i$. \n",
    "\n",
    "\\begin{equation}y_i=\\alpha d_i+x_i\\beta+r_{yi}+\\zeta_i\\end{equation}\n",
    "\n",
    "- We follow notation in [Belloni et al., 2015](https://doi.org/10.1257/jep.28.2.29)\n",
    "- We let $r_{yi}$ be an approximation error (we don't know the functional form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Treatment effects in linear models (2)\n",
    "\n",
    "How to select model, i.e. subset of $x$?\n",
    "\n",
    "- Classic econometrics: \n",
    "  - Use **OLS** and include covariates based on theory or inference\n",
    "  - Problem how to delete covariates systematically? Adjust for multiple hypothesis testing?\n",
    "  \n",
    "- Machine learning:\n",
    "  - Use **LASSO** to perform covariate selection\n",
    "  - Note - estimates are biased towards zero!  \n",
    "    - Problem we omit potentially relevant variables!!\n",
    "    - LASSO excludes  possible confounders if little predictive power $y_i$. \n",
    "    - Excluded variables may still have an effect through $d_i$, e.g. covariates correlated with treatment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixing the LASSO\n",
    "\n",
    "A simple solution suggested by [Belloni et al. (2015)](https://doi.org/10.1257/jep.28.2.29) us to use Post-LASSO to correct for bias:\n",
    "- Step 1: estimate two LASSO models\n",
    "    - a) Regress $y_i$ on $x_i$ \n",
    "    - b) Regress $d_i$ on $x_i$ \n",
    "- Step 2: run OLS using only variables that were kept in either LASSOs\n",
    "\n",
    "What about inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We need further assumptions on sparsity.\n",
    "- See [Chernozhukov et al. (2015)](https://doi.org/10.1257/aer.p20151022) online appendix for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A general solution\n",
    "\n",
    "[Belloni et al. (2015)](https://doi.org/10.1257/jep.28.2.29), [Chernozhukov et al. (2015)](https://doi.org/10.1257/aer.p20151022) write down the two \"prediction equations\":\n",
    "\n",
    "\\begin{align}\n",
    "y_i=&\\alpha d_i+x_i^{\\prime}\\theta_y+r_{yi}+\\zeta_i\\\\\n",
    "d_{i}=&x_{i}^{\\prime} \\theta_{d}+r_{d i}+v_{i}\n",
    "\\end{align}\n",
    "\n",
    "The two equations can be combined into a single structural equation (substite $d_i$ into $y_i$):\n",
    "\n",
    "\\begin{align}y_{i}=&x_{i}^{\\prime}\\left(\\alpha \\theta_{d}+\\theta_{y}\\right)+\\left(\\alpha r_{d i}+r_{y i}\\right)+\\left(\\alpha v_{i}+\\zeta_{i}\\right)\\\\=&x_{i}^{\\prime} \\pi+r_{c i}+\\varepsilon_{i}\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A general solution (2)\n",
    "\n",
    "[Chernozhukov et al. (2015)](https://doi.org/10.1257/aer.p20151022) states the following algorithm for :\n",
    "\n",
    "1. run the two LASSO equations (as in POST-LASSO) and obtain residuals \n",
    "  - $\\hat{\\rho}_i^y$ from $y_i$ on $x_i$\n",
    "  - $\\hat{\\rho}_i^d$ from $d_i$ on $x_i$\n",
    "1. run a regression of $\\hat{\\rho}_i^y$ on $\\hat{\\rho}_i^d$\n",
    "\n",
    "What is the intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Similar to Frisch-Waugh-Lowell where we partial out effects.\n",
    "  - We partial out effect of $x_i$ on both $y_i$ and on $d_i$ seperately \n",
    "- Innovation: We make double selection of variables using LASSO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A general solution (3)\n",
    "\n",
    "[Belloni et al. (2015)](https://doi.org/10.1257/jep.28.2.29) simulates the performance of post selection estimators\n",
    "\n",
    "<center><img src='beh2014_fig1.JPG' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond linear post double selection estimator\n",
    "\n",
    "[Chernozhukov et al. 2018](https://doi.org/10.1111/ectj.12097) that non-linear prediction approaches can be used in prediction steps\n",
    "\n",
    "- tree based inclding random forests, boosted trees\n",
    "- neural networks \n",
    "- kernel models including suppert vector machine \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview: applications of machine learning in econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of ML: for estimation \n",
    "\n",
    "We have seen two major frameworks\n",
    "\n",
    "- causal and generalized random forest \n",
    "- double selection \n",
    "- new applications for time series and diff-and-diff, see references in [Athey and Imbens (2019)](https://doi.org/10.1146/annurev-economics-080217-053433)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of ML: model evaluation\n",
    "\n",
    "Making and gauging predictive models (SDS Intro): \n",
    "\n",
    "- Assess the performance of our models\n",
    "- Choose the parameters that help estimate the best performing model (using validation set)\n",
    "\n",
    "We used two approaches to decomposing models: \n",
    "\n",
    "- Make inference comparing different models'  generalization error (Nadeau and Bengio)\n",
    "- Smart ways of gauging predictive contribution, e.g. SHAP \n",
    "  - Note: we do not expect you to be able to apply this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of ML: new data\n",
    "\n",
    "Machine learning can help us *'fill in the blanks'* and impute missing data. Examples?\n",
    "\n",
    "- E.g. cell phone and smartwatch data\n",
    "    - Inferring poverty ([Blumenstock et al., 2015](https://doi.org/10.1126/science.aac4420))\n",
    "    - Inferring mode of transportation ([Reddy et al., 2010](https://doi.org/10.1145/1689239.1689243); Bjerre-Nielsen et al., 2020)\n",
    "    - Measuring sleep and other behavioral traces \n",
    "\n",
    "Problem: measurement error creates econometric problems \n",
    "- predicted data may have subtle biases\n",
    "- may need to correct for uncertainty \n",
    "    - NOTE: interesting subject for thesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of ML: policy targetting\n",
    "\n",
    "[Kleinberg et al. (2015)](https://doi.org/10.1257/aer.p20151023) argues that in many policy applications we are **mainly** concerned about prediction. Examples: \n",
    "\n",
    "\n",
    "\n",
    "- Short term weather we are (almost) exclusively interested in prediction \n",
    "    - predicting whether it rains  vs. making it rain (causal effect)\n",
    "    - on longer horizon evidence of climate change \n",
    "- Treating knee surgeries\n",
    "  - predict mortality before surgery to avoid treating terminally ill\n",
    "- Audits for taxes\n",
    "  - predicting fraud vs. information campaigns \n",
    "  - behavioral effects in subsequent years\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
